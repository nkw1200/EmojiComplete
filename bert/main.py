
"""BERT_sentiment_sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E0cy1v9xS7e61hxs4tFgVcA1C_qcy51R

#General Sentiment Analysis with BERT and Tensor Flow

##Setup HuggingFace and Transformers

### Requires refactoring to fit our dataset
"""

"""bert-base-uncased will probably give the best results

##Create Dataset - Using Sample Dataset!
"""

from zipfile import ZipFile
import torch
import pandas as pd
import numpy as np
import sys
import os
from tqdm import tqdm
from sklearn.metrics import classification_report
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from datasets import Dataset
import glob

DATA_COLUMN = 'Tweet'
LABEL_COLUMN = 'Emoji'
MODEL_NAME = "distilbert-base-uncased"
DATA_PATH = 'data'
device = 'cuda' if torch.cuda.is_available() else 'cpu'


def main():
    """**Download Data**"""
    zipFile = os.path.join(DATA_PATH, 'dev.zip')
    with ZipFile(zipFile, 'r') as zip:
        zip.extractall()
    all_files = glob.glob(DATA_PATH + "/*.csv")

    li = []

    for filename in all_files:
        temp = pd.read_csv(filename, index_col=None, header=None, names=["Tweet","Emoji"])
        li.append(temp)

    df = pd.concat(li, axis=0, ignore_index=True)

    """# Make Datasets"""

    mappingPath = os.path.join(DATA_PATH, 'mapping.npy')
    mapping_emoji = np.load(mappingPath, allow_pickle=True)

    df[LABEL_COLUMN].replace(mapping_emoji,
                            [i for i in range(len(mapping_emoji))],
                            inplace=True)


    model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, 
                                                                num_labels=len(mapping_emoji))
    model = model.to(device)

    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)


    dataset = Dataset.from_pandas(df)
    dataset = dataset.map(lambda e: tokenizer(e[DATA_COLUMN], 
                                            truncation=True, 
                                            padding='max_length'), batched=True)

    dataset = dataset.rename_column(LABEL_COLUMN, "labels")
    dCols = ['input_ids', 'attention_mask', 'labels']
    dataset.set_format(type='torch', columns=dCols)

    train_testvalid = dataset.train_test_split(test_size=0.1)
    train = train_testvalid["train"]
    test = train_testvalid["test"]

    """# Create the Model

    Further hyperparameter tuning is needed here
    """

    optimizer = torch.optim.Adam(params = model.parameters(), lr=3e-5, eps=1e-08)
    criterion = torch.nn.CrossEntropyLoss()
    dataloader = torch.utils.data.DataLoader(train, batch_size=6)

    # From HuggingFace quickstart
    for epoch in range(3):
        for i, batch in tqdm(enumerate(dataloader)):
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs[0]
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            if i % 10 == 0:
                print(f"loss: {loss}")


    model.eval()

    test_dict = {k: test[k] for k in dCols}
    out = model(**test_dict)
    print(classification_report(test['labels'], torch.max(out['logits'], axis=1)[1]))

    model.save_model('trained-bert')

if __name__ == "__main__":
    main()