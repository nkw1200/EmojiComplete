{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was designed to run under the assumption that shared drive folders for the project are mounted."
      ],
      "metadata": {
        "id": "xbNgkmdd2juc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu2jJDJfDxeC"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqlwkTX9bRqv"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUqFPSFk-07l"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C0zjTr_PR4r"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os, time\n",
        "from google.colab import drive, files\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import SparkSession\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LQAzC2p-PGh"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TZ\"] = \"US/Eastern\"\n",
        "time.tzset()\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DljmoVc_AfB"
      },
      "source": [
        "#Authenticating with Kaggle using kaggle.json\n",
        "Navigate to https://www.kaggle.com. Then go to the Account tab of your user profile and select Create API Token. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
        "\n",
        "Then run the cell below to upload kaggle.json to your Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HHXsJd8_D1K"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yqv0iae_T3P"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download \"rexhaif/emojifydata-en\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGqwGNg_-oS"
      },
      "outputs": [],
      "source": [
        "!mkdir \"kaggle\"\n",
        "!mkdir \"kaggle/input\"\n",
        "!unzip emojifydata-en.zip -d \"kaggle/input\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjFKpdISDtLY"
      },
      "source": [
        "# Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWIWBZbNCwyi"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTDj-rR9DXQY"
      },
      "outputs": [],
      "source": [
        "ss = SparkSession.builder \\\n",
        "    .config(\"spark.driver.memory\", \"10g\") \\\n",
        "    .getOrCreate()\n",
        "sc = ss.sparkContext\n",
        "s  = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_BDLdWqFaR0"
      },
      "source": [
        "Read _.txt as RDD. Change to other files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF_I7XBzFd5X"
      },
      "outputs": [],
      "source": [
        "tweets = sc.textFile(\"./kaggle/input/train.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW9a1tiQFgdE"
      },
      "source": [
        "Convert to python list to concatenate words into tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuayE-AiFknU"
      },
      "outputs": [],
      "source": [
        "tweets_clean = tweets.flatMap(lambda x: x.split(' ')).filter(lambda x:x!='O').filter(lambda x:x!='').filter(lambda x:x!='<STOP>')\n",
        "tweet_list = tweets_clean.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0xV42rnDcN4"
      },
      "outputs": [],
      "source": [
        "tweets = []\n",
        "global counter\n",
        "counter = -1\n",
        "\n",
        "def wordsToTweets(x):\n",
        "    global counter\n",
        "    if(x=='<START>'):\n",
        "        counter+=1\n",
        "        tweets.append('')\n",
        "    else:\n",
        "        tweets[counter]+=(x+\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu-YmM5LDkd0"
      },
      "outputs": [],
      "source": [
        "for word in tweet_list:\n",
        "    wordsToTweets(word)\n",
        "# conver back to rdd\n",
        "tweet_rdd = sc.parallelize(tweets)\n",
        "tweets = tweet_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
        "# get emoji out of text\n",
        "text_rdd = tweets.map(lambda x: (x[0],re.sub(\":.*?:\",\"\",x[1])))\n",
        "emoji_rdd = tweets.map(lambda x: (x[0],re.findall(\":.*?:\",x[1])))\n",
        "emoji_rdd1 = emoji_rdd.map(lambda x:(x[0],x[1][0]))\n",
        "max_emoji = emoji_rdd.map(lambda x: len(x[1])).max()\n",
        "# join text with emoji rdd\n",
        "for i in range(1,max_emoji):\n",
        "    emoji_rdd2 = emoji_rdd.filter(lambda x:len(x[1])>i).map(lambda x: (x[0],x[1][i]))\n",
        "    if i==1:\n",
        "        emoji_rdd3 = emoji_rdd1.union(emoji_rdd2)\n",
        "    else:\n",
        "        emoji_rdd3 = emoji_rdd3.union(emoji_rdd2)\n",
        "\n",
        "rdd_for_df = text_rdd.leftOuterJoin(emoji_rdd3).map(lambda x:(x[1][0][:-1],x[1][1][1:-1]))\n",
        "# create dataframe\n",
        "df = s.createDataFrame(rdd_for_df, ['text','emoji']).distinct()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8sZ_OxZHFe4"
      },
      "outputs": [],
      "source": [
        "df.write.csv('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGT0c0PMNAMQ"
      },
      "outputs": [],
      "source": [
        "!zip -r train.zip train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKfCTB93NONH"
      },
      "outputs": [],
      "source": [
        "!mv ./train.zip ./drive/Shareddrives/EECS\\ 545/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2DljmoVc_AfB",
        "xjFKpdISDtLY"
      ],
      "name": "dataset.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}