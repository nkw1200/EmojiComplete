{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was designed to run under the assumption that shared drive folders for the project are mounted."
      ],
      "metadata": {
        "id": "Xh5oNgto2RGJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0aJVGJMhsFT"
      },
      "source": [
        "# Catch all for repository setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_31f9v-PcwZA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtnhBbghX8-2"
      },
      "outputs": [],
      "source": [
        "# Clone model repository and download the weights for the pretrained DeepMoji model\n",
        "% cd /\n",
        "! unzip -n \"/content/drive/Shareddrives/EECS 545/dev.zip\" -d /content\n",
        "! unzip -n \"/content/drive/Shareddrives/EECS 545/train.zip\" -d /content\n",
        "! unzip -n \"/content/drive/Shareddrives/EECS 545/test.zip\" -d /content\n",
        "\n",
        "% cd \"/content/drive/Shareddrives/EECS 545/DeepMoji\"\n",
        "! git clone https://github.com/huggingface/torchMoji.git\n",
        "! pip install emoji\n",
        "! pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9_7yJwqhxXE"
      },
      "source": [
        "# Uncomment if weights need to be downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0QONm4ux7if"
      },
      "outputs": [],
      "source": [
        "# Download weights from DropBox\n",
        "# Need to say yes here\n",
        "% cd \"/content/drive/Shareddrives/EECS 545/DeepMoji/torchMoji\"\n",
        "# python scripts/download_weights.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxTPnd_YjQBY"
      },
      "source": [
        "# Imports and Dataset setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G25ctXIKhqAy"
      },
      "outputs": [],
      "source": [
        "% cd \"/content/drive/Shareddrives/EECS 545/DeepMoji/torchMoji\"\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from torchmoji.global_variables import NB_TOKENS, PRETRAINED_PATH, VOCAB_PATH, ROOT_PATH\n",
        "import re\n",
        "\n",
        "# These can be replaced by NLTK/glove/BERT for us in the future\n",
        "from torchmoji.word_generator import TweetWordGenerator, WordGenerator # Takes words and splits them\n",
        "from torchmoji.sentence_tokenizer import SentenceTokenizer # Tokenizes via vocab\n",
        "from torchmoji.create_vocab import VocabBuilder # Buils vocab for corpus\n",
        "\n",
        "from torchmoji.model_def import torchmoji_emojis # Model for pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seJf3Q2sjff_"
      },
      "outputs": [],
      "source": [
        "# All the emojis used by DeepMoji out of box\n",
        "\n",
        "EMOJIS = \":joy: :unamused: :weary: :sob: :heart_eyes: \\\n",
        ":pensive: :ok_hand: :blush: :heart: :smirk: \\\n",
        ":grin: :notes: :flushed: :100: :sleeping: \\\n",
        ":relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: \\\n",
        ":sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: \\\n",
        ":neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: \\\n",
        ":v: :sunglasses: :rage: :thumbsup: :cry: \\\n",
        ":sleepy: :yum: :triumph: :hand: :mask: \\\n",
        ":clap: :eyes: :gun: :persevere: :smiling_imp: \\\n",
        ":sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: \\\n",
        ":wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: \\\n",
        ":angry: :no_good: :muscle: :facepunch: :purple_heart: \\\n",
        ":sparkling_heart: :blue_heart: :grimacing: :sparkles:\".split(' ')\n",
        "\n",
        "EMOJIS_set = set([i[1:-1] for i in EMOJIS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HN1TLRj5P6v"
      },
      "outputs": [],
      "source": [
        "train_files = [f for f in os.listdir(\"/content/train\") if os.path.splitext(f)[1]==\".csv\" ]\n",
        "test_files = [f for f in os.listdir(\"/content/test\") if os.path.splitext(f)[1]==\".csv\" ]\n",
        "valid_files = [f for f in os.listdir(\"/content/dev\") if os.path.splitext(f)[1]==\".csv\" ]\n",
        "train_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yhUhWT2jsPZ"
      },
      "outputs": [],
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Added files manually from zip for now\n",
        "train_tweet_df = DataFrame()\n",
        "test_tweet_df = DataFrame()\n",
        "valid_tweet_df = DataFrame()\n",
        "\n",
        "for train_filename in train_files:\n",
        "  temp = pd.read_csv(\"/content/train/\"+train_filename, names=[\"Tweet\",\"Emoji\"])\n",
        "  train_tweet_df = pd.concat([train_tweet_df,temp],ignore_index=True)\n",
        "\n",
        "for test_filename in test_files:\n",
        "  temp = pd.read_csv(\"/content/test/\"+test_filename, names=[\"Tweet\",\"Emoji\"])\n",
        "  test_tweet_df = pd.concat([test_tweet_df,temp],ignore_index=True)\n",
        "\n",
        "for valid_filename in valid_files:\n",
        "  temp = pd.read_csv(\"/content/dev/\"+valid_filename, names=[\"Tweet\",\"Emoji\"])\n",
        "  valid_tweet_df = pd.concat([valid_tweet_df,temp],ignore_index=True)\n",
        "\n",
        "# Map for mapping all possible emojis to numbers, will be useful in the future.\n",
        "emoji_to_number = pd.unique(pd.concat([train_tweet_df[\"Emoji\"],\n",
        "                                      test_tweet_df[\"Emoji\"],\n",
        "                                      valid_tweet_df[\"Emoji\"]],\n",
        "                                      axis = 0))\n",
        "# Testing leftover emoji removal on dataset, comment out later?\n",
        "\n",
        "train_tweet_df['Tweet'] = train_tweet_df['Tweet'].apply(lambda x : emoji.replace_emoji(x, replace=''))\n",
        "test_tweet_df['Tweet'] = test_tweet_df['Tweet'].apply(lambda x : emoji.replace_emoji(x, replace=''))\n",
        "valid_tweet_df['Tweet'] = valid_tweet_df['Tweet'].apply(lambda x : emoji.replace_emoji(x, replace=''))\n",
        "\n",
        "train_tweet_df['Tweet'] = train_tweet_df['Tweet'].apply(lambda x : str(unidecode(x).strip()))\n",
        "test_tweet_df['Tweet'] = test_tweet_df['Tweet'].apply(lambda x : str(unidecode(x).strip()))\n",
        "valid_tweet_df['Tweet'] = valid_tweet_df['Tweet'].apply(lambda x : str(unidecode(x).strip()))\n",
        "\n",
        "train_tweet_df.dropna(how='any', inplace=True)\n",
        "valid_tweet_df.dropna(how='any', inplace=True)\n",
        "test_tweet_df.dropna(how='any', inplace=True)\n",
        "\n",
        "# Create random subset of the data we have due to Colab limitations\n",
        "print(train_tweet_df.shape,test_tweet_df.shape,valid_tweet_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwxHhg1fqUp3"
      },
      "source": [
        "# Common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8SLZqkjqYYg"
      },
      "outputs": [],
      "source": [
        "# Verbose makes the function print the results for given row\n",
        "# If 0 we only print accuracy at the end\n",
        "from sklearn.metrics import f1_score\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "def evaluate_results(X_test, y_test, results,verbose=0):\n",
        "  accuracy = 0.0\n",
        "  top1_accuracy = 0.0\n",
        "  for i in range(len(X_test)):\n",
        "    emoji_id = np.argpartition(results[i], -5)[-5:]\n",
        "    emoji_label = y_test[i]\n",
        "    emoji_id_top_1 = np.argpartition(results[i], -1)[-1:]\n",
        "\n",
        "    ret_string = \"{},\\nActual emoji was :{}:\\nPrediction(s):\".format(X_test[i], \n",
        "                                                              emoji_label\n",
        "                                                              )\n",
        "    for id in emoji_id:\n",
        "      ret_string += EMOJIS[id]\n",
        "    \n",
        "    ret_string += \"\\n\"\n",
        "    ret_string = emoji.emojize(ret_string, use_aliases=True)\n",
        "    \n",
        "    accuracy += 1 if emoji_label.lower() in [EMOJIS[id][1:-1] for id in emoji_id] else 0\n",
        "    top1_accuracy += 1 if emoji_label == EMOJIS[emoji_id_top_1[0]][1:-1] else 0\n",
        "\n",
        "    if verbose !=0:\n",
        "      print(ret_string)\n",
        "      verbose-=1    \n",
        "    top1_accuracy += 1 if emoji_label == EMOJIS[emoji_id_top_1[0]][1:-1] else 0\n",
        "\n",
        "    y_pred.append(EMOJIS[emoji_id_top_1[0]][1:-1])\n",
        "    y_true.append(y_test[i])\n",
        "\n",
        "  print(\"Top 5: \", accuracy/len(X_test)*100, \"%\")\n",
        "  print(\"Top 1:\", top1_accuracy/len(X_test)*100, \"%\")\n",
        "  print(\"F-1: \", f1_score(y_true, y_pred, average = 'weighted'))\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYm7juU5ymj_"
      },
      "outputs": [],
      "source": [
        "def evaluate_results_custom(X_test, y_test, results, mapping,verbose=0):\n",
        "  accuracy = 0\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for i in range(len(X_test)):\n",
        "    emoji_id = np.argpartition(results[i], -3)[-3:]\n",
        "    emoji_id_top_1 = np.argpartition(results[i], -1)[-1:]\n",
        "    \n",
        "    emoji_label = y_test[i]\n",
        "    ret_string = \"{}\\nActual emoji was :{}:\\nPrediction(s):\".format(X_test[i], \n",
        "                                                              emoji_label\n",
        "                                                              )\n",
        "    for id in emoji_id:\n",
        "      ret_string += mapping(id)\n",
        "\n",
        "    ret_string = emoji.emojize(ret_string, use_aliases=True)\n",
        "    accuracy += 1 if emoji_label in {mapping[id][1:-1] for id in emoji_id} else 0\n",
        "    top1_accuracy += 1 if emoji_label == mapping[emoji_id_top_1[0]][1:-1] else 0\n",
        "\n",
        "    y_pred.append(emoji_id_top_1[0])\n",
        "    y_true.append(y_test[i])\n",
        "\n",
        "    if verbose !=0:\n",
        "      print(ret_string)\n",
        "      verbose-=1\n",
        "\n",
        "  print(\"Top 5: \", accuracy/len(X_test)*100, \"%\")\n",
        "  print(\"Top 1:\", top1_accuracy/len(X_test)*100, \"%\")\n",
        "  print(\"F-1: \", f1_score(y_true, y_prd))\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9j5CB7PM_ZA"
      },
      "source": [
        "# Testing new dataset on torchmoji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MmxTAvlUyXj"
      },
      "source": [
        "## Generating Output on Test Data Using Pretrained Weights and Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOnDK003Xs42"
      },
      "outputs": [],
      "source": [
        "# Generate num_test many random data points from original dataset\n",
        "test_tweet_df_temp = test_tweet_df\n",
        "\n",
        "# Split into text and emoji\n",
        "X_test = test_tweet_df_temp[\"Tweet\"].to_numpy()\n",
        "y_test = test_tweet_df_temp[\"Emoji\"].to_numpy()\n",
        "\n",
        "# Max size tweet for tokenization size\n",
        "maxlen_test = len(max(X_test, key=lambda x:len(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcQy-7HqVLA3"
      },
      "outputs": [],
      "source": [
        "# Load pretrained vocabulary to tokenize sentences based on it\n",
        "\n",
        "with open(VOCAB_PATH, 'r') as f:\n",
        "  vocabulary = json.load(f)\n",
        "\n",
        "#Updated implementation of wordgen such that they save a list of rows that\n",
        "# were ignored.\n",
        "# Control+f \"New addition\" in respective file to inspect\n",
        "# Further, removed assertion for the number of sentences input being equal to\n",
        "# the number of sentences found\n",
        "tokenizer = SentenceTokenizer(vocabulary,maxlen_test)\n",
        "\n",
        "# Note, line 119 was changed for below\n",
        "tokenized_X_test = tokenizer.tokenize_sentences(X_test)[0]\n",
        "print(tokenizer.ignored_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4PWA1oAOGv-"
      },
      "outputs": [],
      "source": [
        "# Remove the ignored rows\n",
        "y_test = np.delete(y_test,tokenizer.ignored_rows)\n",
        "X_test = np.delete(X_test,tokenizer.ignored_rows)\n",
        "\n",
        "# Remove the empty rows since tokenized X is ordered based on skipping\n",
        "# bad rows\n",
        "tokenized_X_test = tokenized_X_test[:len(tokenized_X_test)-len(tokenizer.ignored_rows)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVjP92uMMN0K"
      },
      "outputs": [],
      "source": [
        "# Now we can get the probabilities for our sentences on raw DeepMoji\n",
        "m_batches = 5000\n",
        "model = torchmoji_emojis(PRETRAINED_PATH)\n",
        "token_X_batches = np.array_split(tokenized_X_test,m_batches)\n",
        "# Break computation into pieces to save RAM\n",
        "results = [model(curr_batch) for curr_batch in token_X_batches]\n",
        "#results = model(tokenized_X_test)\n",
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P87S65lxprTd"
      },
      "outputs": [],
      "source": [
        "# Flatten all batches\n",
        "results_flat = []\n",
        "for x in results:\n",
        "  for y in x:\n",
        "    results_flat.append(y)\n",
        "len(results_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCpWd5X6MVCb"
      },
      "outputs": [],
      "source": [
        "evaluate_results(X_test, y_test, results_flat,verbose=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTlBS_FFcAhW"
      },
      "source": [
        "## Generating Output on Test Data Using Pretrained Weights and Vocabulary while restricting input to originally trained emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y6Iv2KOcTNk"
      },
      "outputs": [],
      "source": [
        "# Same as earlier occurence, except we now only include samples with emojis\n",
        "# used by DeepMoji\n",
        "test_tweet_df_temp = test_tweet_df[test_tweet_df[\"Emoji\"].str.lower().isin(EMOJIS_set)]\n",
        "\n",
        "X_test = test_tweet_df_temp[\"Tweet\"].to_numpy()\n",
        "y_test = test_tweet_df_temp[\"Emoji\"].to_numpy()\n",
        "\n",
        "maxlen_test = len(max(X_test, key=lambda x:len(x)))\n",
        "print(len(test_tweet_df_temp), len(test_tweet_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ-fmyDEcTNm"
      },
      "outputs": [],
      "source": [
        "with open(VOCAB_PATH, 'r') as f:\n",
        "  vocabulary = json.load(f)\n",
        "\n",
        "tokenizer = SentenceTokenizer(vocabulary,maxlen_test)\n",
        "\n",
        "# Updated implementation of wordgen such that they save a list of rows that\n",
        "# were ignored.\n",
        "# Further, removed assertion for the number of sentences input being equal to\n",
        "# the number of sentences found\n",
        "tokenized_X_test = tokenizer.tokenize_sentences(X_test)[0]\n",
        "print(tokenizer.ignored_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0PRAmZPPz5e"
      },
      "outputs": [],
      "source": [
        "# Remove the ignored rows\n",
        "y_test = np.delete(y_test,tokenizer.ignored_rows)\n",
        "X_test = np.delete(X_test,tokenizer.ignored_rows)\n",
        "\n",
        "# Remove the empty rows since tokenized X is ordered based on skipping\n",
        "# bad rows\n",
        "tokenized_X_test = tokenized_X_test[:len(tokenized_X_test)-len(tokenizer.ignored_rows)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjFeBSiScTNm"
      },
      "outputs": [],
      "source": [
        "# Now we can get the probabilities for our sentences on raw DeepMoji\n",
        "print(len(y_test),len(tokenized_X_test))\n",
        "\n",
        "m_batches = 5000\n",
        "model = torchmoji_emojis(PRETRAINED_PATH)\n",
        "token_X_batches = np.array_split(tokenized_X_test,m_batches)\n",
        "# Break computation into pieces to save RAM\n",
        "results = [model(curr_batch) for curr_batch in token_X_batches]\n",
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/drive/Shareddrives/EECS 545/deepmoji_res_post_remove.npy\", results_flat, allow_pickle=True)"
      ],
      "metadata": {
        "id": "fRMNSyDAWg3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74R-KJb4sf4a"
      },
      "outputs": [],
      "source": [
        "# Flatten all batches\n",
        "results_flat = []\n",
        "for x in results:\n",
        "  for y in x:\n",
        "    results_flat.append(y)\n",
        "len(results_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZVMeEmocTNo"
      },
      "outputs": [],
      "source": [
        "evaluate_results(X_test, y_test, results_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD44a65D_0V8"
      },
      "source": [
        "# Generating output by retraining for our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okiU9ZqwxNMU"
      },
      "outputs": [],
      "source": [
        "from torchmoji.create_vocab import extend_vocab\n",
        "from torchmoji.finetuning import finetune, load_benchmark, finetune\n",
        "from torchmoji.model_def import torchmoji_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9N-ArvD_0WB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "# Map Emojis to numbers\n",
        "train_tweet_df[\"Emoji\"].replace(emoji_to_number,\n",
        "                                [i for i in range(len(emoji_to_number))],\n",
        "                                inplace=True)\n",
        "\n",
        "test_tweet_df[\"Emoji\"].replace(emoji_to_number,\n",
        "                                [i for i in range(len(emoji_to_number))],\n",
        "                                inplace=True)\n",
        "\n",
        "valid_tweet_df[\"Emoji\"].replace(emoji_to_number,\n",
        "                                [i for i in range(len(emoji_to_number))],\n",
        "                                inplace=True)\n",
        "  \n",
        "# Extra cleaning to be safe\n",
        "train_tweet_df['Tweet'] = train_tweet_df['Tweet'][train_tweet_df['Tweet'].str.strip().astype(bool)]\n",
        "test_tweet_df['Tweet'] = test_tweet_df['Tweet'][test_tweet_df['Tweet'].str.strip().astype(bool)]\n",
        "valid_tweet_df['Tweet'] = valid_tweet_df['Tweet'][valid_tweet_df['Tweet'].str.strip().astype(bool)]\n",
        "\n",
        "# Split into text and emoji\n",
        "X_train = train_tweet_df[\"Tweet\"]\n",
        "\n",
        "# Reorder to use accuracy metric or f-1 metric in training.\n",
        "# One hot for F-1, labels for accuracy.\n",
        "y_train = train_tweet_df[\"Emoji\"].to_numpy()\n",
        "y_train = LabelBinarizer().fit_transform(train_tweet_df[\"Emoji\"])\n",
        "\n",
        "X_test = test_tweet_df[\"Tweet\"]\n",
        "y_test = test_tweet_df[\"Emoji\"].to_numpy()\n",
        "y_test = LabelBinarizer().fit_transform(test_tweet_df[\"Emoji\"])\n",
        "\n",
        "X_valid = valid_tweet_df[\"Tweet\"]\n",
        "y_valid = valid_tweet_df[\"Emoji\"].to_numpy()\n",
        "y_valid = LabelBinarizer().fit_transform(valid_tweet_df[\"Emoji\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqQCWF6Ar8SY"
      },
      "outputs": [],
      "source": [
        "# Rebuild Vocabulary\n",
        "ALT_VOCAB_PATH = '{}/model/vocabulary_alt.json'.format(ROOT_PATH)\n",
        "word_gen = WordGenerator(X_train)\n",
        "vocab_builder = VocabBuilder(word_gen)\n",
        "\n",
        "# Line 95 `in word_gen was changed to ignore unicode letters instead of sentence\n",
        "# The function itself seems to be adding unicode not actually\n",
        "\n",
        "# Line 107 needs to become an ascii check\n",
        "# Something is strange though, unidecode returns an ascii string\n",
        "# but the ascii check is for the string class?\n",
        "# My guess is to remove line 137-138\n",
        "vocab_builder.count_all_words()\n",
        "\n",
        "# Fix below must be applied for collab on line 50, if using save_vocab\n",
        "# np_dict = np.array([(i[0],i[1]) for i in self.word_counts.items()], dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4v9Mo2FvzWJ"
      },
      "outputs": [],
      "source": [
        "# Load pretrained vocabulary to tokenize sentences based on it\n",
        "with open(VOCAB_PATH, 'r') as f:\n",
        "  vocabulary = json.load(f)\n",
        "\n",
        "# Extends vocabulary and stores the number of new tokens in num_added\n",
        "# Expected to be 0 for partial dataset\n",
        "num_added = extend_vocab(vocabulary,vocab_builder, max_tokens=10000)\n",
        "num_emojis = len(emoji_to_number)\n",
        "print(\"Total number of data points removed were \", num_added)\n",
        "\n",
        "maxlen_test = len(max(X_test, key=lambda x:len(x)))\n",
        "maxlen_train = len(max(X_train, key=lambda x:len(x)))\n",
        "maxlen_valid = len(max(X_valid, key=lambda x:len(x)))\n",
        "max_len_overall = max([maxlen_train, maxlen_test, maxlen_valid])\n",
        "\n",
        "print(max_len_overall)\n",
        "tokenizer_train = SentenceTokenizer(vocabulary, max_len_overall)\n",
        "tokenizer_test = SentenceTokenizer(vocabulary,max_len_overall)\n",
        "tokenizer_valid = SentenceTokenizer(vocabulary,max_len_overall)\n",
        "\n",
        "# Same error fix in file, the tokenizer here aggressively preprocesses?\n",
        "# Change line 120 to:\n",
        "# assert len(sentences)-2 == next_insert or len(sentences)-1 == next_insert or len(sentences) == next_insert\n",
        "\n",
        "# Tokenize each of the datasets as needed by torchmoji\n",
        "tokenized_X_train = tokenizer_train.tokenize_sentences(X_train)[0]\n",
        "tokenized_X_test = tokenizer_test.tokenize_sentences(X_test)[0]\n",
        "tokenized_X_valid = tokenizer_valid.tokenize_sentences(X_valid)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKmswCJszBO7"
      },
      "outputs": [],
      "source": [
        "# Realign all data\n",
        "print(X_train[tokenizer_train.ignored_rows])\n",
        "print(\"Ignored rows for training\",tokenizer_train.ignored_rows)\n",
        "y_train = np.delete(y_train,tokenizer_train.ignored_rows,0)\n",
        "tokenized_X_train = tokenized_X_train[:len(tokenized_X_train)-len(tokenizer_train.ignored_rows)]\n",
        "\n",
        "print(\"Ignored rows for validating\",tokenizer_valid.ignored_rows)\n",
        "y_valid = np.delete(y_valid,tokenizer_valid.ignored_rows,0)\n",
        "tokenized_X_valid = tokenized_X_valid[:len(tokenized_X_valid)-len(tokenizer_valid.ignored_rows)]\n",
        "\n",
        "print(\"Ignored rows for testing\",tokenizer_test.ignored_rows)\n",
        "y_test = np.delete(y_test,tokenizer_test.ignored_rows,0)\n",
        "tokenized_X_test = tokenized_X_test[:len(tokenized_X_test)-len(tokenizer_test.ignored_rows)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert into format for finetune function in torchMoji\n",
        "input_text =  [tokenized_X_train,tokenized_X_valid,tokenized_X_test]\n",
        "output_label = [y_train,y_valid,y_test]\n",
        "print(y_train.shape, tokenized_X_train.shape)"
      ],
      "metadata": {
        "id": "IOAEXHiWgwtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v4FH9__VeSm"
      },
      "outputs": [],
      "source": [
        "# Generates model for transfer learning provided by authors\n",
        "from torchmoji.class_avg_finetuning import class_avg_finetune\n",
        "from torchmoji.finetuning import finetune\n",
        "\n",
        "num_emojis = len(emoji_to_number)\n",
        "\n",
        "model = torchmoji_transfer(2, \n",
        "                           weight_path=PRETRAINED_PATH,\n",
        "                           extend_embedding=num_added,\n",
        "                           )\n",
        "\n",
        "# finetuning.py:line 526 .numpy()[0] changed\n",
        "# to numpy() due to version change causing \n",
        "# this to directly be a scalar\n",
        "\n",
        "# tested accuracy variant as well as different configurations.\n",
        "# No useful results.\n",
        "# model_def.py:line 249-250 commented out\n",
        "# Update: Above confirmed https://github.com/huggingface/torchMoji/issues/21\n",
        "# Changed line 610 and 611 in finetuning.py for mem\n",
        "print(output_label)\n",
        "model, score =  class_avg_finetune(model,\n",
        "                          input_text,\n",
        "                          output_label, \n",
        "                          num_emojis, \n",
        "                          32,\n",
        "                          'last',\n",
        "                          )\n",
        "\n",
        "results = model(tokenized_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-8chRTA7HBR"
      },
      "outputs": [],
      "source": [
        "  evaluate_results_custom(X_test, y_test, results, emoji_to_number,verbose=100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "C9_7yJwqhxXE",
        "dxTPnd_YjQBY",
        "nwxHhg1fqUp3",
        "W9j5CB7PM_ZA",
        "sTlBS_FFcAhW"
      ],
      "machine_shape": "hm",
      "name": "deepmoji-experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}